!!! note
    
    This overview is under construction - 
    [ v0.0.1 ] May 5, 2023


## Generative Pre-trained Transformer Overview

Parameters
: > In artificial intelligence, a model describes the probability that something will occur. Parameters in a model are the weight of the various probabilities. Tiernan Ray, in an article on GPT-3, described parameters this way:
A parameter is a calculation in a neural network that applies a great or lesser weighting to some aspect of the data, to give that aspect greater or lesser prominence in the overall calculation of the data. It is these weights that give shape to the data, and give the neural network a learned perspective on the data.

---

Neural Network
: > Deep Neural Network are computing systems inspired by the biological neural networks that constitute animal brains. They are based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. 


Generative Pre-Trained Transformers (GPTs)
: > A type of language model that are pre-trained on large amounts of text data

Transformer Architectures
: > Deep neural networks are used to process natural language

GPTs use transformer architectures. GPTs are capable of generating new text in the style of the training data, allowing them to be used for natural language processing tasks such as summarization, question answering, and dialogue generation. GPTs are also able to learn complex relationships between words and phrases, making them useful for more advanced tasks such as machine translation and sentiment analysis.

Parameters are the adjustable weights and biases in a neural network that allow it to learn from data. They are adjusted during the training process, which helps the network learn how to map inputs to outputs more accurately. In other words, parameters enable the network to create relationships between different features of the data, so that it can make better predictions.

Bias in neural networks is the term used to describe the tendency of the network to learn certain patterns or relationships more easily than others. It is related to the weights and parameters of the network, and can be adjusted during training to help the network make more accurate predictions. Bias can be a problem if it leads to overfitting, which means that the network has learned too much from the data, and is not generalizing well to new data.

Weights and parameters of a neural network are the values that determine how the neurons in the network interact with each other. They control the strength of the connections between neurons, as well as the activation functions used for each layer of the network. The weights and parameters are usually initialized randomly, then adjusted during the training process to optimize the performance of the network







>* OpenAI stated that GPT-4 is "more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5. They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively.

>* GPT-4, the latest of those projects, was likely trained using trillions of words of text and many thousands of powerful computer chips. The process cost over $100 million.

>* Generative pre-trained transformers (GPT) are a family of large language models (LLMs). GPT models are artificial neural networks that are based on the transformer architecture, pre-trained on large datasets of unlabelled text, and able to generate novel human-like text. https://en.wikipedia.org/wiki/Generative_pre-trained_transformer

>* Generative Pre-trained Transformers (GPTs) are a type of machine learning model used for natural language processing tasks. These models are pre-trained on massive amounts of data, such as books and web pages, to generate contextually relevant and semantically coherent language.